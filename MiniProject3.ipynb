{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "cKyvxvL896s3",
        "6YalCUoRLPBb"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "tckHih3pnXHo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Mini Project 3 Group 99"
      ]
    },
    {
      "metadata": {
        "id": "MQfGW9ELnweS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Group Member\n",
        "\n",
        "Zhenyuan Ma (ID: 260881867)\n",
        "\n",
        "Yue Xiao (ID: 260740996)\n",
        "\n",
        "Yihe Zhang (ID: 260738383)"
      ]
    },
    {
      "metadata": {
        "id": "9a7WHJo-688F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Connect Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "Ef5lUKctFHG_",
        "colab_type": "code",
        "outputId": "7b712dc9-2077-4a09-92e0-a70cf98377c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xk0yFwiQC6bU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ]
    },
    {
      "metadata": {
        "id": "Dp8lYf_lpHoR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def imgPreprocess (imgSet):\n",
        "    modifiedSet = []\n",
        "    for idx in range(len(imgSet)):\n",
        "        test = imgSet[idx]\n",
        "        ret, thresh = cv.threshold(test, 252, 255, 0, cv.THRESH_BINARY)\n",
        "        ima, contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
        "        \n",
        "        maxC = 0\n",
        "        maxContour = None\n",
        "        for contour in contours:\n",
        "            (x,y,w,h) = cv.boundingRect(contour)\n",
        "            if max(w,h) > maxC:\n",
        "                maxC = max(w,h)\n",
        "                maxContour = contour\n",
        "\n",
        "        (x,y,w,h) = cv.boundingRect(maxContour)\n",
        "        modified = np.zeros((64, 64))\n",
        "        modified[y:y+h+1,x:x+w+1] = thresh[y:y+h+1,x:x+w+1]\n",
        "        modifiedSet.append(modified)\n",
        "    \n",
        "    return np.array(modifiedSet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v1vJX1U67FUx",
        "colab_type": "code",
        "outputId": "836c4daf-6065-4646-b8a3-2ca5e54e68c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2 as cv\n",
        "from keras import utils as np_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "OFFMySa1DOfu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "df_x = np.array(pd.read_pickle('/content/gdrive/My Drive/Comp551Project3/train_images.pkl'), dtype='uint8')\n",
        "df_y = pd.read_csv('/content/gdrive/My Drive/Comp551Project3/train_labels.csv').iloc[:, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cVY_GXoR_uN3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Image preprocess\n",
        "img_x = imgPreprocess(df_x)\n",
        "\n",
        "# Convert to float32 and reshape the images sets to be [samples][width][height][pixels]\n",
        "data_x = df_x.reshape(img_x.shape[0], 64, 64, 1).astype('float32')\n",
        "\n",
        "# Normalize data from 0-255 to 0-1\n",
        "data_x = data_x / 255\n",
        "\n",
        "# One hot encode labels sets\n",
        "data_y = np_utils.to_categorical(df_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cKyvxvL896s3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inception v3"
      ]
    },
    {
      "metadata": {
        "id": "GxrhuxNnn_ap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Model Creation"
      ]
    },
    {
      "metadata": {
        "id": "VwJ7ZhD6-LWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G76d1t6jKWqp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if K.image_data_format() == 'channels_first':\n",
        "    channel_axis = 1\n",
        "else:\n",
        "    channel_axis = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4bXRHfEcKWs9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function to apply convolution and batch normalization.\n",
        "\n",
        "def conv2d_bn(x,\n",
        "              filters,\n",
        "              num_row,\n",
        "              num_col,\n",
        "              padding='same',\n",
        "              strides=(1, 1),\n",
        "              name=None):\n",
        "  \n",
        "    if name is not None:\n",
        "        bn_name = name + '_bn'\n",
        "        conv_name = name + '_conv'\n",
        "    else:\n",
        "        bn_name = None\n",
        "        conv_name = None\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        bn_axis = 1\n",
        "    else:\n",
        "        bn_axis = 3\n",
        "    x = layers.Conv2D(\n",
        "        filters, (num_row, num_col),\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        use_bias=False,\n",
        "        name=conv_name)(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
        "    x = layers.Activation('relu', name=name)(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mqegto_CKWvb",
        "colab_type": "code",
        "outputId": "5ef64e3a-213f-4ccc-c004-c748b2977fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "input_tensor = layers.Input(shape=(64,64,1))\n",
        "\n",
        "x = conv2d_bn(input_tensor, 4, 3, 3, strides=(1, 1), padding='valid')\n",
        "x = conv2d_bn(x, 4, 3, 3, padding='valid')\n",
        "x = conv2d_bn(x, 8, 3, 3, padding='valid')\n",
        "x = layers.MaxPooling2D((3, 3), strides=(1, 1))(x)\n",
        "\n",
        "x = conv2d_bn(x, 12, 1, 1, padding='valid')\n",
        "x = conv2d_bn(x, 32, 3, 3, padding='valid')\n",
        "x = layers.MaxPooling2D((3, 3), strides=(1, 1))(x)\n",
        "\n",
        "\n",
        "branch1x1 = conv2d_bn(x, 8, 1, 1,padding='valid')   # 1,1\n",
        "\n",
        "branch5x5 = conv2d_bn(x, 6, 1, 1,padding='valid') # 1,1\n",
        "branch5x5 = conv2d_bn(branch5x5, 8, 5, 5) # 5,5 \n",
        "\n",
        "branch3x3dbl = conv2d_bn(x, 8, 1, 1,padding='valid') # 1,1\n",
        "branch3x3dbl = conv2d_bn(branch3x3dbl, 12, 3, 3)\n",
        "branch3x3dbl = conv2d_bn(branch3x3dbl, 12, 3, 3)\n",
        "\n",
        "branch_pool = layers.AveragePooling2D((3, 3),  strides=(1, 1),  padding='same')(x)\n",
        "branch_pool = conv2d_bn(branch_pool, 4, 1, 1)\n",
        "x = layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=channel_axis,name='mixed0')\n",
        "\n",
        "\n",
        "branch3x3 = conv2d_bn(x, 32, 3, 3, strides=(2, 2), padding='valid')\n",
        "\n",
        "branch3x3dbl = conv2d_bn(x, 8, 1, 1)\n",
        "branch3x3dbl = conv2d_bn(branch3x3dbl, 12, 3, 3)\n",
        "branch3x3dbl = conv2d_bn(\n",
        "    branch3x3dbl, 12, 3, 3, strides=(2, 2), padding='valid')\n",
        "\n",
        "branch_pool = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "x = layers.concatenate(\n",
        "    [branch3x3, branch3x3dbl, branch_pool],\n",
        "    axis=channel_axis,\n",
        "    name='mixed1')\n",
        "\n",
        "\n",
        "branch1x1 = conv2d_bn(x, 32, 1, 1,padding='valid') # 1 1\n",
        "\n",
        "branch7x7 = conv2d_bn(x, 16, 1, 1,padding='valid') # 1 1\n",
        "branch7x7 = conv2d_bn(branch7x7, 16, 1, 7)\n",
        "branch7x7 = conv2d_bn(branch7x7, 32, 7, 1)\n",
        "\n",
        "branch7x7dbl = conv2d_bn(x, 16, 1, 1,padding='valid') # 1 1\n",
        "branch7x7dbl = conv2d_bn(branch7x7dbl, 16, 7, 1)\n",
        "branch7x7dbl = conv2d_bn(branch7x7dbl, 16, 1, 7)\n",
        "branch7x7dbl = conv2d_bn(branch7x7dbl, 16, 7, 1)\n",
        "branch7x7dbl = conv2d_bn(branch7x7dbl, 32, 1, 7)\n",
        "\n",
        "branch_pool = layers.AveragePooling2D((3, 3),\n",
        "                                      strides=(1, 1 ),\n",
        "                                      padding='same')(x)\n",
        "branch_pool = conv2d_bn(branch_pool, 32, 1, 1,padding='valid') # 1 1\n",
        "x = layers.concatenate(\n",
        "    [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
        "    axis=channel_axis,\n",
        "    name='mixed2')\n",
        "\n",
        "\n",
        "branch3x3 = conv2d_bn(x, 32, 1, 1,padding='valid') # 1 1\n",
        "branch3x3 = conv2d_bn(branch3x3, 64, 3, 3,\n",
        "                      strides=(2, 2), padding='valid')\n",
        "\n",
        "branch7x7x3 = conv2d_bn(x, 32, 1, 1,padding='valid') # 1 1\n",
        "branch7x7x3 = conv2d_bn(branch7x7x3, 32, 1, 7)\n",
        "branch7x7x3 = conv2d_bn(branch7x7x3, 32, 7, 1)\n",
        "branch7x7x3 = conv2d_bn(\n",
        "    branch7x7x3, 32, 3, 3, strides=(2, 2), padding='valid')\n",
        "\n",
        "branch_pool = layers.MaxPooling2D((3, 3), strides=(2, 2),padding='valid')(x) # strides 2 2\n",
        "x = layers.concatenate(\n",
        "    [branch3x3, branch7x7x3, branch_pool],\n",
        "    axis=channel_axis,\n",
        "    name='mixed3')\n",
        "\n",
        "\n",
        "\n",
        "branch1x1 = conv2d_bn(x, 32, 1, 1,padding='valid',name='validcheck') # 1 1\n",
        "\n",
        "branch3x3 = conv2d_bn(x, 32, 1, 1,padding='valid')\n",
        "branch3x3_1 = conv2d_bn(branch3x3, 32, 1, 3)\n",
        "branch3x3_2 = conv2d_bn(branch3x3, 32, 3, 1)\n",
        "branch3x3 = layers.concatenate(\n",
        "    [branch3x3_1, branch3x3_2],\n",
        "    axis=channel_axis,\n",
        "    name='mixed4')\n",
        "\n",
        "branch3x3dbl = conv2d_bn(x, 32, 1, 1,padding='valid')\n",
        "branch3x3dbl = conv2d_bn(branch3x3dbl, 32, 3, 3)\n",
        "branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 32, 1, 3)\n",
        "branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 32, 3, 1)\n",
        "branch3x3dbl = layers.concatenate(\n",
        "    [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)\n",
        "\n",
        "branch_pool = layers.AveragePooling2D(\n",
        "    (3, 3), strides=(1, 1), padding='same')(x)\n",
        "branch_pool = conv2d_bn(branch_pool, 32, 1, 1,padding='valid') # 1 1 \n",
        "x = layers.concatenate(\n",
        "    [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n",
        "    axis=channel_axis,\n",
        "    name='mixed5')\n",
        "        \n",
        "      \n",
        "x = layers.AveragePooling2D(\n",
        "    (3, 3), strides=(1, 1), padding='valid')(x)\n",
        "x = layers.Dropout(0.01)(x)\n",
        "x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "x = layers.Dense(64, activation = \"relu\")(x)\n",
        "x = layers.Dense(10, activation='softmax', name='predictions')(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pKxJJq8yLp1T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Model(input_tensor, x)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lb-ii3VbmG16",
        "colab_type": "code",
        "outputId": "49bf0dcd-f455-4f5f-b532-046f71f1f9f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "cell_type": "code",
      "source": [
        "# Fiting the training set to the model, with epochs = 10\n",
        "modelfit = model.fit(x=data_x, y=data_y, batch_size=100, epochs=10, verbose=1, callbacks=None, validation_split=0.1, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 36000 samples, validate on 4000 samples\n",
            "Epoch 1/10\n",
            "36000/36000 [==============================] - 79s 2ms/step - loss: 1.1247 - acc: 0.6328 - val_loss: 2.4216 - val_acc: 0.4382\n",
            "Epoch 2/10\n",
            "36000/36000 [==============================] - 79s 2ms/step - loss: 0.4467 - acc: 0.8719 - val_loss: 0.5125 - val_acc: 0.8477\n",
            "Epoch 3/10\n",
            "36000/36000 [==============================] - 78s 2ms/step - loss: 0.3327 - acc: 0.9057 - val_loss: 0.9342 - val_acc: 0.7412\n",
            "Epoch 4/10\n",
            "36000/36000 [==============================] - 79s 2ms/step - loss: 0.2717 - acc: 0.9199 - val_loss: 0.4579 - val_acc: 0.8727\n",
            "Epoch 5/10\n",
            "36000/36000 [==============================] - 78s 2ms/step - loss: 0.2324 - acc: 0.9319 - val_loss: 1.1392 - val_acc: 0.6820\n",
            "Epoch 6/10\n",
            "36000/36000 [==============================] - 79s 2ms/step - loss: 0.2012 - acc: 0.9407 - val_loss: 0.4275 - val_acc: 0.8800\n",
            "Epoch 7/10\n",
            "36000/36000 [==============================] - 78s 2ms/step - loss: 0.1792 - acc: 0.9456 - val_loss: 0.4552 - val_acc: 0.8650\n",
            "Epoch 8/10\n",
            "36000/36000 [==============================] - 78s 2ms/step - loss: 0.1583 - acc: 0.9514 - val_loss: 0.6285 - val_acc: 0.8352\n",
            "Epoch 9/10\n",
            "36000/36000 [==============================] - 79s 2ms/step - loss: 0.1452 - acc: 0.9562 - val_loss: 0.3878 - val_acc: 0.8955\n",
            "Epoch 10/10\n",
            "36000/36000 [==============================] - 79s 2ms/step - loss: 0.1333 - acc: 0.9604 - val_loss: 0.2506 - val_acc: 0.9220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xn07juoE_5rT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Data Augmentation"
      ]
    },
    {
      "metadata": {
        "id": "Q2tP1EmqAMYr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "csG-heEpNuul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.15, # Randomly zoom image \n",
        "        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5dhA7WddNu22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen.fit(data_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6nVVIxvoNu-O",
        "colab_type": "code",
        "outputId": "5d2f8d84-c53e-40c0-ec92-b134b1b4309c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "cell_type": "code",
      "source": [
        "# Use cross validation\n",
        "n_splits = 26\n",
        "for _ in range(n_splits):\n",
        "\tx_train, x_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.10)\n",
        "\t# Images augmentation\n",
        "\tmodelfitwithdatagen = model.fit_generator(datagen.flow(x_train, y_train, batch_size=100),\n",
        "                              validation_data=(x_val, y_val),                             \n",
        "                              epochs=5, verbose = 1, steps_per_epoch=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1000/1000 [==============================] - 218s 218ms/step - loss: 0.4679 - acc: 0.8593 - val_loss: 0.3998 - val_acc: 0.8758\n",
            "Epoch 2/5\n",
            "1000/1000 [==============================] - 218s 218ms/step - loss: 0.3897 - acc: 0.8800 - val_loss: 0.2605 - val_acc: 0.9168\n",
            "Epoch 3/5\n",
            "1000/1000 [==============================] - 217s 217ms/step - loss: 0.3472 - acc: 0.8925 - val_loss: 0.2153 - val_acc: 0.9357\n",
            "Epoch 4/5\n",
            "1000/1000 [==============================] - 217s 217ms/step - loss: 0.3218 - acc: 0.9001 - val_loss: 0.1832 - val_acc: 0.9468\n",
            "Epoch 5/5\n",
            "1000/1000 [==============================] - 216s 216ms/step - loss: 0.3056 - acc: 0.9058 - val_loss: 0.2006 - val_acc: 0.9443\n",
            "Epoch 1/5\n",
            "1000/1000 [==============================] - 216s 216ms/step - loss: 0.2923 - acc: 0.9095 - val_loss: 0.1451 - val_acc: 0.9543\n",
            "Epoch 2/5\n",
            "1000/1000 [==============================] - 217s 217ms/step - loss: 0.2748 - acc: 0.9149 - val_loss: 0.2776 - val_acc: 0.9232\n",
            "Epoch 3/5\n",
            "1000/1000 [==============================] - 216s 216ms/step - loss: 0.2668 - acc: 0.9172 - val_loss: 0.2590 - val_acc: 0.9338\n",
            "Epoch 4/5\n",
            "1000/1000 [==============================] - 216s 216ms/step - loss: 0.2568 - acc: 0.9202 - val_loss: 0.1440 - val_acc: 0.9582\n",
            "Epoch 5/5\n",
            "1000/1000 [==============================] - 216s 216ms/step - loss: 0.2485 - acc: 0.9230 - val_loss: 0.1667 - val_acc: 0.9525\n",
            "Epoch 1/5\n",
            "1000/1000 [==============================] - 217s 217ms/step - loss: 0.2432 - acc: 0.9257 - val_loss: 0.1741 - val_acc: 0.9532\n",
            "Epoch 2/5\n",
            "1000/1000 [==============================] - 217s 217ms/step - loss: 0.2354 - acc: 0.9265 - val_loss: 0.1706 - val_acc: 0.9503\n",
            "Epoch 3/5\n",
            "1000/1000 [==============================] - 216s 216ms/step - loss: 0.2309 - acc: 0.9288 - val_loss: 0.1935 - val_acc: 0.9438\n",
            "Epoch 4/5\n",
            "1000/1000 [==============================] - 216s 216ms/step - loss: 0.2252 - acc: 0.9309 - val_loss: 0.1570 - val_acc: 0.9590\n",
            "Epoch 5/5\n",
            "1000/1000 [==============================] - 221s 221ms/step - loss: 0.2188 - acc: 0.9320 - val_loss: 0.1769 - val_acc: 0.9547\n",
            "Epoch 1/5\n",
            "1000/1000 [==============================] - 217s 217ms/step - loss: 0.2226 - acc: 0.9316 - val_loss: 0.0909 - val_acc: 0.9793\n",
            "Epoch 2/5\n",
            "1000/1000 [==============================] - 215s 215ms/step - loss: 0.2159 - acc: 0.9335 - val_loss: 0.0947 - val_acc: 0.9720\n",
            "Epoch 3/5\n",
            "1000/1000 [==============================] - 221s 221ms/step - loss: 0.2101 - acc: 0.9353 - val_loss: 0.1746 - val_acc: 0.9583\n",
            "Epoch 4/5\n",
            "1000/1000 [==============================] - 218s 218ms/step - loss: 0.2093 - acc: 0.9349 - val_loss: 0.1035 - val_acc: 0.9700\n",
            "Epoch 5/5\n",
            "1000/1000 [==============================] - 217s 217ms/step - loss: 0.2082 - acc: 0.9370 - val_loss: 0.0912 - val_acc: 0.9748\n",
            "Epoch 1/5\n",
            "1000/1000 [==============================] - 218s 218ms/step - loss: 0.2062 - acc: 0.9368 - val_loss: 0.1244 - val_acc: 0.9620\n",
            "Epoch 2/5\n",
            "1000/1000 [==============================] - 217s 217ms/step - loss: 0.2008 - acc: 0.9386 - val_loss: 0.1317 - val_acc: 0.9653\n",
            "Epoch 3/5\n",
            "1000/1000 [==============================] - 217s 217ms/step - loss: 0.2021 - acc: 0.9378 - val_loss: 0.1038 - val_acc: 0.9718\n",
            "Epoch 4/5\n",
            "1000/1000 [==============================] - 218s 218ms/step - loss: 0.1963 - acc: 0.9394 - val_loss: 0.1135 - val_acc: 0.9693\n",
            "Epoch 5/5\n",
            "1000/1000 [==============================] - 217s 217ms/step - loss: 0.1913 - acc: 0.9420 - val_loss: 0.1206 - val_acc: 0.9655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bhG19RDjB019",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Output"
      ]
    },
    {
      "metadata": {
        "id": "RIvi4rKcRO6B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load and reshape the test iamges\n",
        "df_test = np.array(pd.read_pickle('/content/gdrive/My Drive/Comp551Project3/test_images.pkl'), dtype='uint8')\n",
        "data_test = df_test.reshape(df_test.shape[0], 64, 64, 1).astype('float32')\n",
        "# Normalize test data from 0-255 to 0-1\n",
        "x_test = data_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hMAznxc-RP-k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Predict lables\n",
        "pred = model.predict(x_test)\n",
        "pred_val = np.argmax(pred,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C9bIWHfNR3nL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Write\n",
        "pd.DataFrame(pred_val, columns=['Category']).to_csv(\"./submission_36000_140.csv\", index_label='Id')\n",
        "pd.DataFrame(pred_val, columns=['Category']).to_csv(\"/content/gdrive/My Drive/submission_36000_140.csv\", index_label='Id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "D874g_41fvED"
      },
      "cell_type": "markdown",
      "source": [
        "##CNN"
      ]
    },
    {
      "metadata": {
        "id": "WSjX40dfOByF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Lambda\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "K.set_image_dim_ordering('th')\n",
        "from keras.preprocessing import image\n",
        "gen = image.ImageDataGenerator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6YalCUoRLPBb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the model"
      ]
    },
    {
      "metadata": {
        "id": "ae0TrQ5TNk6g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reshape to be [samples][pixels][width][height]\n",
        "data2_x = df_x.reshape(img_x.shape[0], 1, 64, 64).astype('float32')\n",
        "\n",
        "# Normalize data from 0-255 to 0-1\n",
        "data2_x = data2_x / 255\n",
        "\n",
        "# Split data\n",
        "X_train = data2_x[:32000]\n",
        "Y_train = data_y[:32000]\n",
        "\n",
        "X_validation = data2_x[32000:]\n",
        "Y_validation = data_y[32000:]\n",
        "\n",
        "num_classes = Y_validation.shape[1]\n",
        "\n",
        "batches = gen.flow(X_train, Y_train)\n",
        "val_batches = gen.flow(X_validation, Y_validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e33JobeQKKr6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(5, 5),activation='relu',kernel_initializer='he_normal',input_shape=(1, 64, 64)))\n",
        "    model.add(Conv2D(32, kernel_size=(5, 5),activation='relu',kernel_initializer='he_normal'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.20))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu',padding='same',kernel_initializer='he_normal'))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu',padding='same',kernel_initializer='he_normal'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu',padding='same',kernel_initializer='he_normal'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(), metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gERpIGIWi9dt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build the model\n",
        "model = model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ka0C8X48LT2w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "450 steps per epoch & 50 validation steps\n",
        "\n",
        "26 epochs"
      ]
    },
    {
      "metadata": {
        "id": "nwR0yiEuLoNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(generator=batches, steps_per_epoch=450, epochs=2,\n",
        "               validation_data=val_batches, validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CfQK0VezLq-n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_validation, Y_validation, verbose=0)\n",
        "print(\"Prediction: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}